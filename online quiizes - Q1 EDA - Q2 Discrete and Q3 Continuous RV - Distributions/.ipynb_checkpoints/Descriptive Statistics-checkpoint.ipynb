{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "6b27cb0f04aa50f19a3c38337bf88b305364a46c"
   },
   "source": [
    "# Statistics\n",
    "\n",
    "__Application__ - Measuring the performance of a computer system\n",
    "\n",
    "\\begin{enumerate}\n",
    "\\item How should you report the performance as a single number?\n",
    "\\item Is specifying the mean the correct way to summarize a sequence of measurements? \n",
    "\\item How should you report the variability of measured quantities? \n",
    "\\item What are the alternatives to variance and when are they appropriate? \n",
    "\\item How should you interpret the variability? \n",
    "\\item How much confidence can you put on data with a large variability? \n",
    "\\item  How many measurements are required to get a desired level of statistical confidence? \n",
    "\\item How should you summarize the results of several different workloads on a single computer system? \n",
    "\\item How should you compare two or more computer systems using several different workloads? \n",
    "\\item Is comparing the mean performance sufficient? \n",
    "\\item What model best describes the relationship between two variables? Also, how good is the model? \n",
    "\\end{enumerate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Concepts\n",
    "\n",
    "1.  __Independent Events__: Two events are called independent if the occurrence of one event does not in any way affect the probability of the other event. Thus, knowing that one event has occurred does not in any way change our estimate of the probability of the other event. \n",
    "2.  __Random Variable__: A variable is called a random variable if it takes one of a specified set of values with a specified probability. \n",
    "3.  __Cumulative Distribution Function__: The Cumulative Distribution Function (CDF) of a random variable maps a given value $a$ to the probability of the variable taking a value less than or equal to $a$: \n",
    "$$F_x(a) = P(x \\leq a)$$ \n",
    "\n",
    "4.  __Probability Density Function__: The derivative  \n",
    "$$\n",
    "f(x)=\\frac{dF(x)}{dx}\n",
    "$$\n",
    "of the CDF $F(x)$ is called the probability density function (pdf) of $x$. Given a pdf $f(x)$, the probability of $x$ being in the interval $(x_1, x_2)$ can also be computed by integration: \n",
    "\n",
    "$$\n",
    "P\\left(x_{1}<x \\leq x_{2}\\right)=F\\left(x_{2}\\right)-F\\left(x_{1}\\right)=\\int_{x_{1}}^{x_{2}} f(x) d x\n",
    "$$\n",
    "\n",
    "\n",
    "5.  __Probability Mass Function__: For discrete random variable, the CDF is not continuous and, therefore, not differentiable. In such cases, the probability mass function (pmf) is used in place of pdf. Consider a discrete random variable $x$ that can take $n$ distinct values ${x_1, x_2, \\ldots, x_n}$ with probabilities ${p_1, p_2, \\ldots, p_n}$ such that the probability of the ith value $x_i$ is $p_i$. The pmf maps $x_i$ to $p_i$: \n",
    "$$f(x_i)=p_i$$.\n",
    "The probability of $x$ being in the interval $(x_1, x_2)$ can also be computed by summation: \n",
    "\n",
    "$$\n",
    "P\\left(x_{1}<x \\leq x_{2}\\right)=F\\left(x_{2}\\right)-F\\left(x_{1}\\right)=\\sum_{i \\atop x_{1}<x_{i} \\leq x_{2}} p_{i}\n",
    "$$\n",
    "\n",
    "6.  __Mean or Expected Value__: \n",
    "\n",
    "$$\n",
    "\\operatorname{Mean} \\mu=E(x)=\\sum_{i=1}^{n} p_{i} x_{i}=\\int_{-\\infty}^{+\\infty} xf(x)dx\n",
    "$$\n",
    "Summation is used for discrete and integration for continuous variables, respectively. \n",
    "\n",
    "7.  __Variance__: The quantity $(x – \\mu)^2$ represents the square of distance between $x$ and its mean. The expected value of this quantity is called the variance $x$: \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(x)=E\\left[(x-\\mu)^{2}\\right]=\\sum_{i=1}^{n} p_{i}\\left(x_{i}-\\mu\\right)^{2}=\\int_{-\\infty}^{+\\infty}\\left(x_{i}-\\mu\\right)^{2}f(x)dx\n",
    "$$\n",
    "\n",
    "\n",
    "The variance is traditionally denoted by $\\sigma^2$. The square root of the variance is called the standard deviation and is denoted by $\\sigma$. \n",
    "\n",
    "\n",
    "8.  __Coefficient of Variation__: The ratio of the standard deviation to the mean is called the Coefficient of Variation (C.O.V.): \n",
    "\n",
    "$$\n",
    "\\text { C.O.V. }=\\frac{\\text { standard deviation }}{\\text { mean }}=\\frac{\\sigma}{\\mu}\n",
    "$$\n",
    "\n",
    "\n",
    "9.  __Covariance__: Given two random variables $x$ and $y$ with means $\\mu_x$ and $\\mu_y$, their covariance is \n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(x, y)=\\sigma_{x y}^{2}=E\\left[\\left(x-\\mu_{x}\\right)\\left(y-\\mu_{y}\\right)\\right]=E(x y)-E(x) E(y)\n",
    "$$\n",
    "\n",
    "\n",
    "For independent variables, the covariance is zero since \n",
    "$$E(xy) = E(x)E(y)$$  \n",
    "\n",
    "\n",
    "Although independence always implies zero covariance, the reverse is not true. It is possible for two variables to be dependent and still have zero covariance. \n",
    "\n",
    "10.  __Correlation Coefficient__: The normalized value of covariance is called the correlation coefficient or simply the correlation \n",
    "\n",
    "$$\n",
    "\\text { Correlation }(x, y)=\\rho_{x y}=\\frac{\\sigma_{x y}^{2}}{\\sigma_{x} \\sigma_{y}}\n",
    "$$\n",
    "\n",
    "The correlation always lies between -1 and +1. \n",
    "\n",
    "11.  __Mean and Variance of Sums__: If $x_1, x_2, \\ldots, x_k$ are $k$ random variables and if $a_1, a_2, \\ldots, a_k$ are $k$ arbitrary constants (called weights), then \n",
    "\n",
    "\n",
    "\\begin{array}{c}{E\\left(a_{1} x_{1}+a_{2} x_{2}+\\ldots+a_{k} x_{k}\\right)=a_{1} E\\left(x_{1}\\right)+} \\\\ {a_{2} E\\left(x_{2}\\right)+\\ldots+a_{k} E\\left(x_{k}\\right)}\\end{array}\n",
    "\n",
    "\n",
    "\n",
    "For independent variables, \n",
    "\n",
    "\n",
    "\\begin{array}{l}{\\operatorname{Var}\\left(a_{1} x_{1}+a_{2} x_{2}+\\ldots+a_{k} x_{k}\\right)=a^{2} \\operatorname{Var}\\left(x_{1}\\right)} \\\\ {\\quad+a_{2}^{2} \\operatorname{Var}\\left(x_{2}\\right)+\\ldots+a_{k}^{2} \\operatorname{Var}\\left(x_{k}\\right)}\\end{array}\n",
    "\n",
    "\n",
    "12.  __Quantile__: The x value at which the CDF takes a value $α$ is called the $\\alpha$-quantile or 100$\\ alpha$-percentile. It is denoted by $x_\\alpha$ and is such that the probability of $x$ being less than or equal to $x_\\alpha$ is $\\alpha$: \n",
    "\n",
    "$$\n",
    "P\\left(x \\leq x_{\\alpha}\\right)=F\\left(x_{\\alpha}\\right)=\\alpha\n",
    "$$\n",
    "\n",
    "\n",
    "13.  __Median__: The 50-percentile (or 0.5-quantile) of a random variable is called its median. \n",
    "\n",
    "14.  __Mode__: The most likely value, that is, $x_i$, that has the highest probability $p_i$, or the $x$ at which pdf is maximum, is called the mode of x. \n",
    "\n",
    "15.  __Normal Distribution__: This is the most commonly used distribution in data analysis. The sum of a large number of independent observations from any distribution has a normal distribution. Also known as Gaussian distribution, its pdf is given by \n",
    "\n",
    "$$\n",
    "f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-(x-\\mu)^{2} / 2 \\sigma^{2}}, \\quad-\\infty \\leq x \\leq+\\infty\n",
    "$$\n",
    "\n",
    "There are two parameters $\\mu$ and $\\sigma$, which are also the mean and standard deviations of x. A normal variate is denoted by $N(\\mu, \\sigma)$. A normal distribution with zero mean and unit variance is called a __unit normal__ or __standard normal distribution__ and is denoted as $N(0, 1)$. In statistical modeling, you will frequently need to use quantiles of the unit normal distribution. An $\\alpha$-quantile of a unit normal variate $z ~ N(0, 1)$ is denoted by $z_\\alpha$. If a random variable x has a $z \\sim N(\\mu, \\sigma)$ distribution, then $(x - \\mu)/\\sigma$ has a $N(0, 1)$ distribution. Thus,\n",
    "\n",
    "$$\n",
    "P\\left(\\frac{x-\\mu}{\\sigma} \\leq z_{\\alpha}\\right)=\\alpha\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "P\\left(x \\leq \\mu+z_{\\alpha} \\sigma\\right)=\\alpha\n",
    "$$\n",
    "The areas under the unit normal pdf between 0 and z for various values of z are listed in various tables in statistics books or following python function:\n",
    "\n",
    "    from math import *\n",
    "    def phi(x):\n",
    "        #'Cumulative distribution function for the standard normal distribution'\n",
    "        return (1.0 + erf(x / sqrt(2.0))) / 2.0\n",
    "\n",
    "There are two main reasons for the popularity of the normal distribution:\n",
    "\n",
    "(a)  The sum of n independent normal variates is a normal variate. If $x_i \\sim  N(\\mu_i, \\sigma_i)$, then $x=\\sum_{i=1}^{n} a_{i} x_{i}$ has a normal distribution with mean $\\mu=\\sum_{i=1}^{n} a_{i} \\mu_{i}$ and variance $\\sigma^{2}=\\sum_{i=1}^{n} a^{2} \\sigma_{i}^{2}$ . As a result of this linearity property, normal processes remain normal after passing through linear systems, which are popular in electrical engineering.\n",
    "(b)  The sum of a large number of independent observations from any distribution tends to have a normal distribution. This result, which is called the __central limit theorem__, is true for observations from all distributions. As a result of this property, experimental errors, which are contributed by many factors, are modeled with a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Value of a Random Variable\n",
    "\n",
    "Let $X$ be a random variable with a finite number of finite outcomes $${\\displaystyle x_{1},x_{2},\\ldots ,x_{k}}$$ occurring with probabilities $${\\displaystyle p_{1},p_{2},\\ldots ,p_{k},}$$ respectively. The expectation of $X$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{E}[X]=\\sum_{i=1}^{k} x_{i} p_{i}=x_{1} p_{1}+x_{2} p_{2}+\\cdots+x_{k} p_{k}\n",
    "$$\n",
    "\n",
    "Since all probabilities $p_{i}$ add up to 1 $({\\displaystyle p_{1}+p_{2}+\\cdots +p_{k}=1})$, the expected value is the weighted average, with $p_{i}$’s being the weights.\n",
    "\n",
    "If all outcomes $x_{i}$ are equiprobable (that is, ${\\displaystyle p_{1}=p_{2}=\\cdots =p_{k}})$, then the weighted average turns into the simple average. If the outcomes $x_{i}$ are not equiprobable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others.\n",
    "\n",
    "### Variation and Standard Deviation of a Random Variable\n",
    "\n",
    "\n",
    "The formula for the sample standard deviation is\n",
    "\n",
    "$$\n",
    "s=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "where ${\\displaystyle \\textstyle \\{x_{1},\\,x_{2},\\,\\ldots ,\\,x_{N}\\}}$ are the observed values of the sample items, $\\textstyle {\\bar {x}}$ is the mean value of these observations, and $N$ is the number of observations in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main reasons for the popularity of the normal distribution:\n",
    "\n",
    "(a)  The sum of $n$ independent normal variates is a normal variate. If $x_i \\approx N(\\mu_i, \\sigma_i)$, then $x=\\sum_{i=1}^{n} a_{i} x_{i}$ has a normal distribution with mean $\\mu=\\sum_{i=1}^{n} a_{i} \\mu_{i}$ and variance $\\sigma^{2}=\\sum_{i=1}^{n} a_{i}^{2} \\sigma_{i}^{2}$. As a result of this linearity property, normal processes remain normal after passing through linear systems, which are popular in electrical engineering.\n",
    "(b)  The sum of a large number of independent observations from any distribution tends to have a normal distribution. This result, which is called the __central limit theorem__, is true for observations from all distributions. As a result of this property, experimental errors, which are contributed by many factors, are modeled with a normal distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d398fc1cdab0835e4eab20c78f721a0d22233cc"
   },
   "source": [
    "Descriptive statistics are measures that summarize important features of data, often with a single number. Producing descriptive statistics is a common first step to take after cleaning and preparing a data set for analysis. We've already seen several examples of deceptive statistics in earlier lessons, such as means and medians. In this lesson, we'll review some of these functions and explore several new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARIZING DATA BY A SINGLE NUMBER\n",
    "In the most condensed form, a single number may be presented that gives the key characteristic of the data set. This single number is usually called an __average__ of the data. To be meaningful, this average should be representative of a major part of the data set. Three popular alternatives to summarize a sample are to specify its __mean, median, or mode__. These measures are what statisticians call __indices of central tendencies__. The name is based on the fact that these measures specify the center of location of the distribution of the observations in the sample.\n",
    "\n",
    "__Sample mean__ is obtained by taking the sum of all observations and dividing this sum by the number of observations in the sample. __Sample median__ is obtained by sorting the observations in an increasing order and taking the observation that is in the middle of the series. If the number of observations is even, the mean of the middle two values is used as a median. __Sample mode__ is obtained by plotting a histogram and specifying the midpoint of the bucket where the histogram peaks. For categorical variables, mode is given by the category that occurs most frequently.\n",
    "\n",
    "The word __sample__ in the names of these indices signifies the fact that the values obtained are based on just one sample. However, if it is clear from the context that the discussion is about a single sample, and there is no ambiguity, the shorter names __mean, median, and mode__ can be used.\n",
    "\n",
    "Mean and median always exist and are unique. Given any set of observations, the mean and median can be determined. Mode, on the other hand, may not exist. An example of this would be if all observations were equal. In addition, even if modes exist, they may not be unique. There may be more than one mode, that is, there may be more than one local peak in the histogram.\n",
    "\n",
    "<img src=\"figure12.1.png\">\n",
    "\n",
    "FIGURE 1  Five distributions showing relationships among mean, median, and mode.\n",
    "\n",
    "\n",
    "The three indices are generally different. Figure 1 shows five different pdf’s. Distribution (a) has a unimodal, symmetrical pdf. In this case, the mode exists with the mean, median, and mode being equal. Distribution (b) has a bimodal, symmetrical pdf. In this case, the mode is not unique. The median and mean are equal. Distribution (c) is a uniform density function. There is no mode and the mean and median are equal. Distribution (d) has a pdf skewed to the right (with a tail toward the right). For this distribution, the value of the mean is greater than the median, which in turn is greater than the mode. Finally, distribution (e) has a pdf skewed to the left; that is, it has a tail on the left. In this case, the mean is less than the median, which is less than the mode.\n",
    "\n",
    "The main problem with the mean is that it is affected more by outliers than the median or mode. A single outlier can make a considerable change in the mean. This is particularly true for small samples. Median and mode are resistant to several outlying observations.\n",
    "\n",
    "The mean gives equal weight to each observation and in this sense makes full use of the sample. Median and mode ignore a lot of the information.\n",
    "\n",
    "The mean has an additivity or linearity property in that the mean of a sum is a sum of the means. This does not apply to the mode or median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARIZING VARIABILITY\n",
    "\n",
    "_Then there is the man who drowned crossing a stream with an average depth of six inches._\n",
    "\n",
    "— W. I. E. Gates\n",
    "\n",
    "Given a data set, summarizing it by a single number is rarely enough. It is important to include a statement about its variability in any summary of the data. This is because given two systems with the same mean performance, one would generally prefer one whose performance does not vary much from the mean. For example, Figure 3 shows histograms of the response times of two systems. Both have the same mean response time of 2 seconds. In case (a), the response time is always close to its mean value, while in case (b), the response time can be 1 millisecond sometimes and 1 minute at other times. Which system would you prefer? Most people would prefer the system with low variability. \n",
    "\n",
    "<img src=\"Figure12.3.png\">\n",
    "FIGURE 12.3  Histograms of response times of two systems.\n",
    "\n",
    "Variability is specified using one of the following measures, which are called indices of dispersion:\n",
    "\n",
    "*  Range — minimum and maximum of the values observed \n",
    "*  Variance or standard deviation \n",
    "*  10- and 90-percentiles \n",
    "*  Semi-interquantile range \n",
    "*  Mean absolute deviation \n",
    "\n",
    "\n",
    "The range of a stream of values can be easily calculated by keeping track of the minimum and the maximum. The variability is measured by the difference between the maximum and the minimum. The larger the difference, the higher the variability. In most cases, the range is not very useful. The minimum often comes out to be zero and the maximum comes out to be an “outlier” far from typical values. Unless there is a reason for the variable to be bounded between two values, the maximum goes on increasing with the number of observations, the minimum goes on decreasing with the number of observations, and there is no “stable” point that gives a good indication of the actual range. The conclusion is that the range is useful if and only if there is a reason to believe that the variable is bounded. The range gives the best estimate of these bounds.\n",
    "\n",
    "The variance of a sample of n observations ${x_1, x_2, \\dots, x_n}$ is calculated as follows:\n",
    "\n",
    "$$\n",
    "s^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^2 \\quad \\text { where } \\quad \\bar{x}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\n",
    "$$\n",
    "\n",
    "The quantity $s^2$ is called the **sample variance** and its square root s is called the **sample standard deviation**. The word sample can be dropped if there is no ambiguity and it it is clear from the context that the quantities refer to just one sample. Notice that in computing the variance, the sum of squares  is divided by $n - 1$ and not n. This is because only $n - 1$ of the n differences  are independent. Given $n - 1$ differences, the nth difference can be computed since the sum of all $n$ differences must be zero. The number of independent terms in a sum is also called its *degrees of freedom*.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In practice, the main problem with variance is that it is expressed in units that are the square of the units of the observations. For example, the variance of response time could be 4 seconds squared or 4,000,000 milliseconds squared. Changing the unit of measurement has a squared effect on the numerical magnitude of the variance. For this reason, it is preferable to use the standard deviation. It is in the same unit as the mean, which allows us to compare it with the mean. Thus, if the mean response time is 2 seconds and the standard deviation is 2 seconds, there is considerable variability. On the other hand, a standard deviation of 0.2 second for the same mean would be considered small. In fact, the ratio of standard deviation to the mean, or the coefficient of variation (C.O.V.), is even better because it takes the scale of measurement (unit of measurement) out of variability consideration. A C.O.V. of 5 is large, and a C.O.V. of 0.2 (or 20%) is small no matter what the unit is. \n",
    "\n",
    "Percentiles are also a popular means of specifying dispersion. Specifying the 5-percentile and the 95-percentile of a variable has the same impact as specifying its minimum and maximum. However, it can be done for any variable, even for variables without bounds. When expressed as a fraction between 0 and 1 (instead of a percentage), the percentiles are also called quantiles. Thus 0.9-quantile is the same as 90-percentile. \n",
    "\n",
    "Another term used is fractile, which is synonymous with quantile. The percentiles at multiples of 10% are called deciles. Thus, the first decile is 10-percentile, the second decile is 20-percentile, and so on. Quartiles divide the data into four parts at 25, 50, and 75%. Thus, 25% of the observations are less than or equal to the first quartile $Q_1$, 50% of the observations are less than or equal to the second quartile $Q_2$, and 75% are less than or equal to the third quartile $Q_3$. Notice that the second quartile $Q_2$ is also the median. The $\\alpha$-quantiles can be estimated by sorting the observations and taking the $[(n-1)\\alpha+1]$th element in the ordered set. Here, $[.]$ is used to denote rounding to the nearest integer. For quantities exactly halfway between two integers, use the lower integer.\n",
    "\n",
    "The range between $Q_3$ and $Q_1$ is called the interquartile range of the data. One half of this range is called Semi-Interquartile Range (SIQR), that is,\n",
    "\n",
    "$$\n",
    "\\operatorname{SIQR}=\\frac{Q_{3}-Q_{1}}{2}=\\frac{x_{0.75}-x_{0.25}}{2}\n",
    "$$\n",
    "\n",
    "Another measure of dispersion is the mean absolute deviation, which is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text { Mean absolute deviation }=\\frac{1}{n} \\sum_{i=1}^{n}\\left|x_{i}-x\\right|\n",
    "$$\n",
    "\n",
    "The key advantage of the mean absolute deviation over the standard deviation is that no multiplication or square root is required. \n",
    "\n",
    "Among the preceding indices of dispersion, the range is affected considerably by outliers. The sample variance is also affected by outliers, but the effect is less than that on the range. The mean absolute deviation is next in resistance to outliers. The semi-interquantile range is very resistant to outliers. It is preferred to the standard deviation for the same reasons that the median is preferred to the mean. Thus, if the distribution is highly skewed, outliers are highly likely and the SIQR is more representative of the spread in the data than the standard deviation. In general, the SIQR is used as an index of dispersion whenever the median is used as an index of central tendency.\n",
    "\n",
    "Finally, it should be mentioned that all of the preceding indices of dispersion apply only for quantitative data. For qualitative (categorical) data, the dispersion can be specified by giving the number of most frequent categories that comprise the given percentile, for instance, the top 90%.\n",
    "\n",
    "\\begin{example}\n",
    "In an experiment, which was repeated 32 times, the measured CPU time was found to be {3.1, 4.2, 2.8, 5.1, 2.8, 4.4, 5.6, 3.9, 3.9, 2.7, 4.1, 3.6, 3.1, 4.5, 3.8, 2.9, 3.4, 3.3, 2.8, 4.5, 4.9, 5.3, 1.9, 3.7, 3.2, 4.1, 5.1, 3.2, 3.9, 4.8, 5.9, 4.2}. The sorted set is {1.9, 2.7, 2.8, 2.8, 2.8, 2.9, 3.1, 3.1, 3.2 3.2, 3.3, 3.4, 3.6, 3.7, 3.8, 3.9, 3.9, 3.9, 4.1, 4.1, 4.2, 4.2, 4.4, 4.5, 4.5, 4.8, 4.9, 5.1, 5.1, 5.3, 5.6, 5.9}. Then \n",
    "\n",
    "    The 10-percentile is given by [1 + (31)(0.10)] = 4th element = 2.8. \n",
    "      \n",
    "    The 90-percentile is given by [1 + (31)(0.90)] = 29th element = 5.1.\n",
    "    \n",
    "    The first quartile $Q_1$ is given by [1 + (31)(0.25)] = 9th element = 3.2. \n",
    "    \n",
    "    The median $Q_2$ is given by [1 + (31)(0.50)] = 16th element = 3.9. \n",
    "    \n",
    "    The third quartile $Q_3$ is given by [1 + (31)(0.75)] = 24th element = 4.5. \n",
    "\\end{example}\n",
    "\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\mathrm{SIOR}=\\frac{Q_{3}-Q_{1}}{2}=\\frac{4.5-3.2}{2}=0.65\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Observations \n",
    "Given: A sample ${x_1, x_2, \\dots, x_n}$ of n observations. \n",
    "1.  Sample arithmetic mean:  $\\bar{x}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}$\n",
    "2.  Sample geometric mean:  $\\dot{x}=\\left(\\prod_{i=1}^{n} x_{i}\\right)^{1 / n}$\n",
    "3.  Sample harmonic mean:  $x=\\frac{n}{\\frac{1}{x_{1}}+\\frac{1}{x_{2}}+\\cdots+\\frac{1}{x_{n}}}$\n",
    "4.  Sample median: $$\\left\\{\\begin{array}{ll}{x_{((n-1) / 2)}} & {\\text { if } n \\text { is odd }} \\\\ {0.5\\left(x_{(n / 2)}+x_{((1+n) / 2)}\\right)} & {\\text { otherwise }}\\end{array}\\right.$$\n",
    "Here x(i) is the ith observation in the sorted set. \n",
    "5.  Sample mode = observation with the highest frequency (for categorical data). \n",
    "6.  Sample variance:  $s^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}$\n",
    "7.  Sample standard deviation:  $s=\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}$\n",
    "8.  Coefficient of variation =  $\\frac{s}{\\bar{x}}$\n",
    "9.  Coefficient of skewness =  $=\\frac{1}{n s^{3}} \\sum_{i=1}^{n}\\left(x_{i}-\\bar x\\right)^{3}$\n",
    "10.  Range: Specify the minimum and maximum. \n",
    "11.  Percentiles: 100p-percentile  \n",
    "12.  Semi-interquartile range  $\\mathrm{SIQR}=\\frac{Q_{3}-Q_{1}}{2}=\\frac{x_{0.75}-x_{025}}{2}$\n",
    "13.  Mean absolute deviation $=\\frac{1}{n} \\sum_{i=1}^{n}\\left|x_{i}-\\bar{x}\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0ab09074c07755d5d14181422190357d56495c5"
   },
   "source": [
    "# Measures of Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "234e96a71a62e388dbf652599825d60b1737ac55"
   },
   "source": [
    "Measures of center are statistics that give us a sense of the \"middle\" of a numeric variable. In other words, centrality measures give you a sense of a typical value you'd expect to see. Common measures of center include the mean, median and mode.\n",
    "\n",
    "The mean is simply an average: the sum of the values divided by the total number of records. As we've seen before, there are several ways to get means in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a61a7b8319f16fcea30cd08646fe3031cc17918"
   },
   "outputs": [],
   "source": [
    "cars <- mtcars      # Use the mtcars data set\n",
    "\n",
    "mean(cars$mpg)      # mean() gets the mean for 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fbdf03d5cf71b853f4588b412730900b022118c"
   },
   "outputs": [],
   "source": [
    "# colMeans() gets the means for all columns in a data frame\n",
    "colMeans(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6753e8438a00b6282b618b0ee9139cfc3bc7befc"
   },
   "outputs": [],
   "source": [
    "# rowMeans() gets the means for all rows in a data frame\n",
    "head(rowMeans(cars))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38f958d09409b208162b6e1050e4968982a07114"
   },
   "source": [
    "The median of a distribution is the value where 50% of the data lies below it and 50% lies above it. In essence, the median splits the data in half. The median is also known as the 50% percentile since 50% of the observations are found below it. As we've seen previously, you can get the median using the median() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7b8a3149488d0bab80fef024b6d64bb9f27b1cb"
   },
   "outputs": [],
   "source": [
    "median(cars$mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc8f6c9fd4d1e55c532c84f27c380461b8a14ccf"
   },
   "source": [
    "To get the median of every column, we can use the apply() function which takes a data object, a function to execute, and a specified margin (rows or columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d19032425872731489f81a8abcb07ee3120b33d"
   },
   "outputs": [],
   "source": [
    "colMedians <- apply(cars,            \n",
    "                    MARGIN=2,        # Operate on columns\n",
    "                    FUN = median)    # Use function median\n",
    "\n",
    "colMedians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "274b12c48cee89b0e9516d255b761a2205b1737e"
   },
   "source": [
    "Although the mean and median both give us some sense of the center of a distribution, they aren't always the same. The median always gives us a value that splits the data into two halves while the mean is a numeric average so extreme values can have a significant impact on the mean. In a symmetric distribution, the mean and median will be the same. Let's investigate with a density plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27fc6cb8ee4342d095358c6e3cb3650334cb3c78"
   },
   "outputs": [],
   "source": [
    "norm_data <- rnorm(100000)          # Generate normally distributed data\n",
    "\n",
    "plot(density(norm_data))            # Create a density plot\n",
    "\n",
    "abline(v=mean(norm_data), lwd=5)    # Plot a thick black line at the mean\n",
    "\n",
    "abline(v=median(norm_data), col=\"red\", lwd=2 )   # Plot a red line at the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60ccc9174a99dec68aa3a2fc41a73b0347d63aac"
   },
   "source": [
    "In the plot above the mean and median are both so close to zero that the red median line lies on top of the thicker black line drawn at the mean.\n",
    "\n",
    "In skewed distributions, the mean tends to get pulled in the direction of the skew, while the median tends to resist the effects of skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9a9faa17eb448f5761268cdb33192a7b92d9b950"
   },
   "outputs": [],
   "source": [
    "skewed_data <- rexp(100000,1)           # Generate skewed data\n",
    "\n",
    "plot(density(skewed_data), xlim=c(0,4))    \n",
    "\n",
    "# Black line at the mean\n",
    "abline(v=mean(skewed_data), lwd=5)  \n",
    "\n",
    "# Red line at the median\n",
    "abline(v=median(skewed_data), col=\"red\", lwd=2 )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cf2b622481819a94308ca565a1d9a68cc666b5"
   },
   "source": [
    "The mean is also influenced heavily by outliers while the median resists the influence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee2e7fab8279247ce44a2882f1990af594c79a78"
   },
   "outputs": [],
   "source": [
    "norm_data <- rnorm(50)             # Generate 50 normally distributed points\n",
    "\n",
    "outliers <- rnorm(3, mean=15)      # Generate 3 outliers\n",
    "\n",
    "norm_data <- c(norm_data,outliers)      # Add outliers\n",
    "\n",
    "plot(density(norm_data))                \n",
    "\n",
    "# Black line at the mean\n",
    "abline(v=mean(norm_data), lwd=5)        \n",
    "\n",
    "# Red line at the median\n",
    "abline(v=median(norm_data), col=\"red\", lwd=2 )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7d2b1374c8135272a1913cd3a92ea7cf91d06b8"
   },
   "source": [
    "Since the median tends to resist the effects of skewness and outliers, it is known a \"robust\" statistic. The median generally gives a better sense of the typical value in a distribution with significant skew or outliers.\n",
    "The mode of a variable is simply the value that appears most frequently. Unlike mean and median, you can take the mode of a categorical variable and it is possible to have multiple modes. R does not include a function to find the mode, since it is not always a particularly useful statistic: oftentimes all the values in variable are unique so the mode is essentially meaningless. You can find the mode of a variable by creating a data table for the variable to get the counts of each value and then getting the variable with the largest count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eac7dafb2ee536c4171ad4932c78c3b6a5d6cd92"
   },
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "data <- c(\"cat\",\"hat\",\"cat\",\"hat\",\"hat\",\"sat\")   \n",
    "\n",
    "# Create table of counts\n",
    "data_table <- table(data)                   \n",
    "\n",
    "data_table\n",
    "\n",
    "# Get the index of the variable with the max count\n",
    "max_index <- which.max(data_table)   \n",
    "\n",
    "# Use the index to get the mode from the table's names\n",
    "names(data_table)[max_index]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a836af96fa689e8d1f284c2ba43754cb937a06c"
   },
   "source": [
    "If you need to repeatedly find the mode, you could wrap these steps into a user-defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f081ed8b378dc021fda5f3a3f473e9a0caa26207"
   },
   "outputs": [],
   "source": [
    "mode_function <- function(data){                         # Define new function\n",
    "    data_table <- table(data)                            # Create data table\n",
    "    max_index <- which.max(data_table)                   # Find max index\n",
    "    if (is.numeric(data)){                               # If input is numeric data\n",
    "        return(as.numeric(names(data_table)[max_index])) # Return output as numeric\n",
    "    }\n",
    "    names(data_table)[max_index]            # Otherwise return output as character\n",
    "}\n",
    "\n",
    "mode_function(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83e63a496a34853fec3a3cdff8b065a089474416"
   },
   "source": [
    "*Note: Base R contains a function called mode() but it does not find the mode of a data set: it checks the type or storage mode of an object.*\n",
    "\n",
    "Let's use our new mode function to find the modes of each column of the mtcars data set by passing it in to the apply function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "603e3271b5c9f559fddbf5d1c10e1e917430919a"
   },
   "outputs": [],
   "source": [
    "colModes <- apply(cars,            \n",
    "                 MARGIN=2,               # operate on columns\n",
    "                 FUN = mode_function)    # use function mode_function\n",
    "\n",
    "print(colModes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2d0d75518dfdff2906d6ae059a5b562e392a500"
   },
   "source": [
    "# Measures of Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "713fcb357c8b38c1fdc9930d8edf982a3daf5ef6"
   },
   "source": [
    "Measures of spread (dispersion) are statistics that describe how data varies. While measures of center give us an idea of the typical value, measures of spread give us a sense of how much the data tends to diverge from the typical value.\n",
    "\n",
    "One of the simplest measures of spread is the range. Range is the distance between the maximum and minimum observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a3064ef7b09529f8af35f8fbb1c9b35fe3892e1"
   },
   "outputs": [],
   "source": [
    "# Subtract min from max to get the range\n",
    "max(cars$mpg) - min(cars$mpg)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f13afa5b7e5ffc4a6e92aab25d72a8a881c4c3d"
   },
   "source": [
    "As noted earlier, the median represents the 50th percentile of a data set. A summary of several percentiles can be used to describe a variable's spread. We can extract the minimum value (0th percentile), first quartile (25th percentile), median, third quartile(75th percentile) and maximum value (100th percentile) using the quantile() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0da6afb1ef627e14628f5663cf09cb1923bd96ae"
   },
   "outputs": [],
   "source": [
    "quantile(cars$mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8853e57104a82f92245f6f43a545eda3ccb605e7"
   },
   "source": [
    "Since these values are so commonly used to describe data, they are known as the \"five number summary\" and R has a couple other ways to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "372fc8726b13d00561676563822f4326c6e0d78d"
   },
   "outputs": [],
   "source": [
    "# Get five number summary\n",
    "fivenum(cars$mpg)   \n",
    "\n",
    "# Summary() shows the five number summary plus the mean\n",
    "summary(cars$mpg)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3dc6fcc84b6f0c76a17447277a7af9f35a57338c"
   },
   "source": [
    "The quantile() function also lets you check percentiles other than common ones that make up the five number summary. To find percentiles, pass a vector of percentiles to the probs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "060b84b04b69c5f26cbc518d0fea7d51b4c333e2"
   },
   "outputs": [],
   "source": [
    "quantile(cars$mpg,\n",
    "        probs = c(0.1,0.9))  # get the 10th and 90th percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41dfa78b98d864c8f9dfa4d8795f844c27c241da"
   },
   "source": [
    "Interquartile (IQR) range is another common measure of spread. IQR is the distance between the 3rd quartile and the 1st quartile, which encompasses half the data. R has a built in IRQ() fuction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7e0e387db9d4c619e0d9c99d1e379f00882c2e4"
   },
   "outputs": [],
   "source": [
    "IQR(cars$mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "917801fbdf5f61ade38700d95cdf977b1d246f27"
   },
   "source": [
    "The boxplots we learned to create in the lessons on plotting are just visual representations of the five number summary and IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6b7a8bedf4abd12642fdefa882e623e4092dec1"
   },
   "outputs": [],
   "source": [
    "five_num <- fivenum(cars$mpg)\n",
    "\n",
    "boxplot(cars$mpg)\n",
    "\n",
    "text(x=five_num[1], adj=2, labels =\"Minimum\")\n",
    "text(x=five_num[2], adj=2.3, labels =\"1st Quartile\")\n",
    "text(x=five_num[3], adj=3, labels =\"Median\")\n",
    "text(x=five_num[4], adj=2.3, labels =\"3rd Quartile\")\n",
    "text(x=five_num[5], adj=2, labels =\"Maximum\")\n",
    "text(x=five_num[3], adj=c(0.5,-8), labels =\"IQR\", srt=90, cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1aec52e8c0a558bdb03a8694c03a7dca18ff2aef"
   },
   "source": [
    "Variance and standard deviation are two other common measures of spread. The variance of a distribution is the average of the squared deviations (differences) from the mean. Use the built-in function var() to check variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a61d9278ad75dfcb18991bad68b9eecb8e1c9247"
   },
   "outputs": [],
   "source": [
    "var(cars$mpg)   # get variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1283f354dd789c569f00f888fee06de507cf8293"
   },
   "source": [
    "The standard deviation is the square root of the variance. Standard deviation can be more interpretable than variance, since the standard deviation is expressed in terms of the same units as the variable in question while variance is expressed in terms of units squared. Use sd() to check the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a60d6cb530e003081871b77a836fffd5a4dfda82"
   },
   "outputs": [],
   "source": [
    "sd(cars$mpg)    # get standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4628a9719e803670402ae35f3eee8132cd790ff7"
   },
   "source": [
    "Since variance and standard deviation are both derived from the mean, they are susceptible to the influence of data skew and outliers. Median absolute deviation is an alternative measure of spread based on the median, which inherits the median's robustness against the influence of skew and outliers. Use the built in mad() function to find median absolute deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f9dfe3da3d9b048db1828cf179fdf61605c4acb"
   },
   "outputs": [],
   "source": [
    "mad(cars$mpg)    # get median absolute deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3df13c4e5b7c85fe9181c7ef20c0c950ef048140"
   },
   "source": [
    "# Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1536137c4a667c99ee16bdbbaa644e1575501dd9"
   },
   "source": [
    "Beyond measures of center and spread, descriptive statistics include measures that give you a sense of the shape of a distribution. Skewness measures the skew or asymmetry of a distribution while kurtosis measures how much data is in the tails of a distribution v.s. the center. We won't go into the exact calculations behind skewness and kurtosis, but they are essentially just statistics that take the idea of variance a step further: while variance involves squaring deviations from the mean, skewness involves cubing deviations from the mean and kurtosis involves raising deviations from the mean to the 4th power.\n",
    "\n",
    "To check skewness and kurtosis, we'll need the \"e1071\" package. First let's create some some dummy data and inspect it with a series of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da72f488102007b582fb037dabf42a552f55f710"
   },
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "\n",
    "normal_data <- rnorm(100000)                       # Generate normally distributed data\n",
    "skewed_data <- c(rnorm(35000,sd=2)+2,rexp(65000))  # Generate skewed data\n",
    "uniform_data <- runif(100000,0,1)                  # Generate uniformly distributed data\n",
    "peaked_data <- c(rexp(100000),                     # Generate data with a sharp peak\n",
    "                (rexp(100000)*-1))\n",
    "\n",
    "\n",
    "par(mfrow=c(2,2))                          # Make density plots of the distributions*\n",
    "plot(density(normal_data))\n",
    "plot(density(skewed_data),xlim=c(-5,5))\n",
    "plot(density(uniform_data))\n",
    "plot(density(peaked_data),xlim=c(-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1223693a78a0718b73fbd25d73f68b72a3173c1"
   },
   "source": [
    "*Note: par() lets you set various graphical parameters. In this case, mfrow=c(2,2) lets us combine 4 plots into a single plot with 2 rows and 2 columns.*\n",
    "\n",
    "Now let's check the skewness of each of the distributions. Since skewness measures asymmetry, we'd expect to see low skewness for all of the distributions except for the second one, because all the others are roughly symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3aa3e6b3375d7b8328565a0a1cb01f5a31465e8"
   },
   "outputs": [],
   "source": [
    "skewness(normal_data)\n",
    "skewness(skewed_data)\n",
    "skewness(uniform_data)\n",
    "skewness(peaked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74d2dd97962ee2573b306ce4a311ac580269b1a1"
   },
   "source": [
    "The 3 roughly symmetric distributions have almost zero skewness, while the positively skewed distribution has positive skewness.\n",
    "Now let's check kurtosis. Since kurtosis measures peakedness, we'd expect the flat (uniform) distribution have low kurtosis while the distributions with sharper peaks should have higher kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c61a19f7a773b966ccf0f0b1a5863d7b3fdc08e"
   },
   "outputs": [],
   "source": [
    "kurtosis(normal_data)\n",
    "kurtosis(skewed_data)\n",
    "kurtosis(uniform_data)\n",
    "kurtosis(peaked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a484a49aeb9770f0d79139e6315d3b39a6e9585c"
   },
   "source": [
    "As we can see from the output, the normally distributed data has a kurtosis near zero, the flat distribution has negative kurtosis and the two distributions with more data in the tails v.s. the center have higher kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9159de47ff5118d21d3812570d8686c02d9358ce"
   },
   "source": [
    "# Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "978fdecc4d601fb953b470b84c2c70e326adeae8"
   },
   "source": [
    "Descriptive statistics help you explore features of your data, like center, spread and shape by summarizing them with numerical measurements. Descriptive statistics help inform the direction of an analysis and let you communicate your insights to others quickly and succinctly. In addition, certain values, like the mean and variance, are used in all sorts of statistical tests and predictive models.\n",
    "\n",
    "In this lesson we generated a lot of random data to illustrate concepts, but we haven't actually learned much about the functions we've been using to generate random data. In the next lesson, we'll learn about probability distributions, including how to draw random data from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7075033a9026b2cdef262a3b0e2d2e54d8794748"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe5062574a10cf5cb69021a70ab6f93e17d5732a"
   },
   "source": [
    "# Exercises\n",
    "\n",
    "To do the exercises, fork this notebook and then fill in and run the code boxes according to the exercise instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1158418a5bd691da1378c34049d31dfd9b85f94"
   },
   "source": [
    "### Exercise #1\n",
    "Load the Titanic training data set and then calculate the difference between the mean and median of the Fare column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "fff1a2a0dac63895b39a0cae7f0543c38a3a65b2"
   },
   "outputs": [],
   "source": [
    "titanic_train <- read.csv(\"../input/train.csv\")\n",
    "\n",
    "\"Your Code Here!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "915483ee807b092e2eda52f068594383ba7b6b90"
   },
   "source": [
    "### Exercise #2\n",
    "Calculate the standard deviation of the Fare column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "227db923fbed173b47e9f477ed2b57a1e2b50d97"
   },
   "outputs": [],
   "source": [
    "\"Your Code Here!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56f3469f0f92c59b3265893d99f2fa4603e84273"
   },
   "source": [
    "### Exercise #3\n",
    "Find the mode of the Fare column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "e61bc093042a8c19f4f176f2b06a58b463a7c160"
   },
   "outputs": [],
   "source": [
    "fare_table <- table(titanic_train$Fare)\n",
    "\n",
    "\"Your Code Here!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
